# main_simple.py - Vers√£o Simplificada (Apenas MTCNN)
"""
WellMind Vision API - Computer Vision Simplified

Modelos de Deep Learning Utilizados:
1. MTCNN (Multi-task Cascaded Convolutional Networks) - Detec√ß√£o facial

Esta vers√£o simplificada foca em detec√ß√£o facial robusta sem depend√™ncia
de TensorFlow pesado, ideal para desenvolvimento e testes r√°pidos.
"""

from fastapi import FastAPI, File, UploadFile, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import cv2
import numpy as np
from datetime import datetime
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Inicializar FastAPI
app = FastAPI(
    title="WellMind Vision API - Simplified",
    description="""
    # Computer Vision API for Face Detection
    
    ## Deep Learning Model:
    - **MTCNN**: Multi-task Cascaded Convolutional Neural Networks
    
    ## Capabilities:
    - ‚úì Face Detection
    - ‚úì Facial Landmarks (5 points)
    - ‚úì Confidence Scoring
    - ‚úì Multi-face Detection
    - ‚úì Basic Emotion Estimation (rule-based)
    
    Vers√£o simplificada sem DeepFace para testes r√°pidos.
    """,
    version="1.0.0-simple"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Carregar MTCNN
try:
    from mtcnn import MTCNN
    face_detector = MTCNN()
    logger.info("‚úì MTCNN loaded successfully")
except Exception as e:
    logger.error(f"‚úó Error loading MTCNN: {e}")
    face_detector = None

# Modelos Pydantic
class FaceDetectionResponse(BaseModel):
    faces_detected: int = Field(..., description="N√∫mero de faces detectadas")
    face_locations: List[Dict[str, Any]] = Field(..., description="Localiza√ß√£o das faces")
    confidence: float = Field(..., description="Confian√ßa m√©dia das detec√ß√µes")
    timestamp: str = Field(..., description="Timestamp ISO 8601")
    model_used: str = Field(..., description="Modelo utilizado")

class SimpleEmotionResponse(BaseModel):
    detected_faces: int
    estimated_mood: str
    confidence: float
    stress_level: str
    recommendations: List[str]
    timestamp: str
    note: str

# Fun√ß√µes auxiliares
def decode_image(file_bytes: bytes) -> np.ndarray:
    """Decodifica bytes para imagem OpenCV"""
    try:
        nparr = np.frombuffer(file_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        return img
    except Exception as e:
        logger.error(f"Error decoding image: {e}")
        return None

def estimate_mood_from_geometry(keypoints: dict, confidence: float) -> tuple:
    """
    Estimativa b√°sica de humor baseada em geometria facial
    (Simplificado - n√£o substitui an√°lise real de emo√ß√µes)
    """
    # Extrair pontos
    left_eye = keypoints['left_eye']
    right_eye = keypoints['right_eye']
    mouth_left = keypoints['mouth_left']
    mouth_right = keypoints['mouth_right']
    
    # Calcular √¢ngulo da boca (simplificado)
    mouth_slope = (mouth_right[1] - mouth_left[1]) / (mouth_right[0] - mouth_left[0] + 1e-6)
    
    # Estimativa b√°sica
    if abs(mouth_slope) < 0.05:
        mood = "neutral"
        stress = "Medium"
    elif mouth_slope < -0.05:
        mood = "positive"  # Boca "sorrindo"
        stress = "Low"
    else:
        mood = "negative"  # Boca "triste"
        stress = "High"
    
    # Ajustar por confian√ßa
    if confidence < 0.7:
        stress = "Medium"
    
    return mood, stress

def generate_basic_recommendations(mood: str, stress: str) -> List[str]:
    """Recomenda√ß√µes baseadas em estimativa"""
    recommendations = []
    
    if mood == "positive":
        recommendations.append("‚úÖ Humor positivo detectado - Continue assim!")
        recommendations.append("üåü Compartilhe sua energia positiva com a equipe")
    elif mood == "negative":
        recommendations.append("‚ö†Ô∏è Considere fazer uma pausa de 10 minutos")
        recommendations.append("üßò Pratique exerc√≠cios de respira√ß√£o")
    else:
        recommendations.append("üåø Mantenha o equil√≠brio e autocuidado")
    
    if stress == "High":
        recommendations.append("üí¨ Fale com seu gestor sobre a carga de trabalho")
        recommendations.append("üò¥ Verifique sua qualidade de sono")
    
    return recommendations

async def verify_token(authorization: Optional[str] = Header(None)):
    """Auth simplificado"""
    return True

# ==================== ENDPOINTS ====================

@app.get("/", tags=["Root"])
async def root():
    """Endpoint raiz"""
    return {
        "service": "WellMind Vision API - Simplified",
        "version": "1.0.0-simple",
        "status": "operational",
        "description": "Vers√£o simplificada com MTCNN apenas",
        "endpoints": {
            "health": "/health",
            "detect_face": "/api/v1/vision/detect-face",
            "analyze_simple": "/api/v1/vision/analyze-simple",
            "documentation": "/docs"
        }
    }

@app.get("/health", tags=["Health"])
async def health_check():
    """Health check"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "models": {
            "mtcnn": "loaded" if face_detector else "error"
        },
        "note": "Vers√£o simplificada - MTCNN apenas"
    }

@app.post("/api/v1/vision/detect-face", response_model=FaceDetectionResponse, tags=["Vision AI"])
async def detect_face(
    file: UploadFile = File(..., description="Imagem com face (JPEG, PNG)"),
    authenticated: bool = Depends(verify_token)
):
    """
    # Detec√ß√£o Facial com MTCNN
    
    Detecta faces e retorna:
    - Localiza√ß√£o (bounding box)
    - Pontos de refer√™ncia (olhos, nariz, boca)
    - Confian√ßa da detec√ß√£o
    
    Modelo: MTCNN (Multi-task Cascaded CNN)
    """
    try:
        # Validar arquivo
        if not file.content_type.startswith('image/'):
            raise HTTPException(
                status_code=400,
                detail=f"Tipo de arquivo inv√°lido: {file.content_type}"
            )
        
        # Ler imagem
        contents = await file.read()
        img = decode_image(contents)
        
        if img is None:
            raise HTTPException(status_code=400, detail="Imagem inv√°lida")
        
        if face_detector is None:
            raise HTTPException(status_code=500, detail="Detector n√£o dispon√≠vel")
        
        # Detectar faces
        logger.info("Executando detec√ß√£o MTCNN...")
        faces = face_detector.detect_faces(img)
        
        if not faces:
            raise HTTPException(
                status_code=400,
                detail="Nenhuma face detectada. Use uma foto com rosto vis√≠vel."
            )
        
        # Processar faces detectadas
        face_locations = []
        confidences = []
        
        for idx, face in enumerate(faces):
            box = face['box']
            keypoints = face['keypoints']
            conf = face['confidence']
            confidences.append(conf)
            
            face_locations.append({
                "face_id": idx,
                "bounding_box": {
                    "x": int(box[0]),
                    "y": int(box[1]),
                    "width": int(box[2]),
                    "height": int(box[3])
                },
                "confidence": float(conf),
                "landmarks": {
                    "left_eye": [int(keypoints['left_eye'][0]), int(keypoints['left_eye'][1])],
                    "right_eye": [int(keypoints['right_eye'][0]), int(keypoints['right_eye'][1])],
                    "nose": [int(keypoints['nose'][0]), int(keypoints['nose'][1])],
                    "mouth_left": [int(keypoints['mouth_left'][0]), int(keypoints['mouth_left'][1])],
                    "mouth_right": [int(keypoints['mouth_right'][0]), int(keypoints['mouth_right'][1])]
                }
            })
        
        avg_confidence = sum(confidences) / len(confidences)
        
        logger.info(f"‚úì {len(faces)} face(s) detectada(s) com confian√ßa m√©dia: {avg_confidence:.2f}")
        
        return FaceDetectionResponse(
            faces_detected=len(faces),
            face_locations=face_locations,
            confidence=avg_confidence,
            timestamp=datetime.utcnow().isoformat(),
            model_used="MTCNN (Multi-task Cascaded CNN)"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro na detec√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/vision/analyze-simple", response_model=SimpleEmotionResponse, tags=["Vision AI"])
async def analyze_simple(
    file: UploadFile = File(...),
    authenticated: bool = Depends(verify_token)
):
    """
    # An√°lise Simplificada de Humor
    
    Estimativa b√°sica baseada em:
    - Geometria facial (MTCNN landmarks)
    - Confian√ßa da detec√ß√£o
    - Regras heur√≠sticas
    
    **Nota**: Esta √© uma vers√£o simplificada. Para an√°lise precisa
    de emo√ß√µes, use a vers√£o completa com DeepFace.
    """
    try:
        contents = await file.read()
        img = decode_image(contents)
        
        if img is None or face_detector is None:
            raise HTTPException(status_code=400, detail="Imagem ou detector inv√°lido")
        
        faces = face_detector.detect_faces(img)
        
        if not faces:
            raise HTTPException(status_code=400, detail="Nenhuma face detectada")
        
        # Usar face com maior confian√ßa
        face = max(faces, key=lambda x: x['confidence'])
        confidence = face['confidence']
        keypoints = face['keypoints']
        
        # Estimativa de humor (simplificada)
        mood, stress = estimate_mood_from_geometry(keypoints, confidence)
        
        # Gerar recomenda√ß√µes
        recommendations = generate_basic_recommendations(mood, stress)
        
        return SimpleEmotionResponse(
            detected_faces=len(faces),
            estimated_mood=mood,
            confidence=confidence,
            stress_level=stress,
            recommendations=recommendations,
            timestamp=datetime.utcnow().isoformat(),
            note="Estimativa baseada em geometria facial - Use vers√£o completa para an√°lise precisa"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro na an√°lise: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    logger.info("Iniciando WellMind Vision API (Simplified)...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
